{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a3378f-12e5-4257-8765-4d2de3e52c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Data Shape: (50,)\n",
      "Labels Shape: (50,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load EEG data (Assumed shape: (samples, time_steps, 4))\n",
    "eeg_data = np.load(\"muse_eeg_data.npy\", allow_pickle=True)\n",
    "\n",
    "# Load labels (Assumed shape: (samples,))\n",
    "eeg_labels = np.load(\"muse_eeg_labels.npy\")\n",
    "\n",
    "# Print shapes\n",
    "print(\"EEG Data Shape:\", eeg_data.shape)   # (samples, time_steps, 4)\n",
    "print(\"Labels Shape:\", eeg_labels.shape)   # (samples,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e277ed2-e433-4d71-8c9a-086f8d672510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-47.8515625 , -40.0390625 , -57.6171875 , -91.30859375],\n",
       "       [-38.0859375 ,  -4.39453125, -23.92578125, -84.9609375 ],\n",
       "       [-30.2734375 , -11.71875   , -22.4609375 , -70.3125    ],\n",
       "       ...,\n",
       "       [-50.78125   , -26.85546875, -30.2734375 , -85.9375    ],\n",
       "       [-65.91796875, -21.97265625, -27.83203125, -85.44921875],\n",
       "       [-61.03515625, -10.25390625, -18.06640625, -93.75      ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42da3d6e-015c-4373-9e8d-8dd1cdd5b780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: <class 'numpy.ndarray'>, Shape: (2557, 4)\n",
      "Sample 1: <class 'numpy.ndarray'>, Shape: (2568, 4)\n",
      "Sample 2: <class 'numpy.ndarray'>, Shape: (2568, 4)\n",
      "Sample 3: <class 'numpy.ndarray'>, Shape: (2568, 4)\n",
      "Sample 4: <class 'numpy.ndarray'>, Shape: (2568, 4)\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(eeg_data[:5]):  # Check first 5 samples\n",
    "    print(f\"Sample {i}: {type(sample)}, Shape: {np.array(sample).shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8f26d0-4e6d-4b6f-a9fd-b625f7b94b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time_steps = min(sample.shape[0] for sample in eeg_data)  # Find shortest\n",
    "eeg_data = np.array([sample[:min_time_steps] for sample in eeg_data])  # Truncate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53288a7-fd0b-4c07-bbea-90f08a8e6fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: <class 'numpy.ndarray'>, Shape: (2556, 4)\n",
      "Sample 1: <class 'numpy.ndarray'>, Shape: (2556, 4)\n",
      "Sample 2: <class 'numpy.ndarray'>, Shape: (2556, 4)\n",
      "Sample 3: <class 'numpy.ndarray'>, Shape: (2556, 4)\n",
      "Sample 4: <class 'numpy.ndarray'>, Shape: (2556, 4)\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(eeg_data[:5]):  # Check first 5 samples\n",
    "    print(f\"Sample {i}: {type(sample)}, Shape: {np.array(sample).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2752ce84-1490-4149-89ba-e014a0af92af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2556, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958f0097-5ae1-4b4f-beb0-fc84d5ade6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2556</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2556</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2556</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2556</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2556</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2556\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m672\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2556\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2556\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m10,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2556\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2556\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m165\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,021</span> (226.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,021\u001b[0m (226.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,829</span> (225.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m57,829\u001b[0m (225.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, LSTM, Dense, Dropout, Input\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (2556, 4)  # (timesteps, features)\n",
    "num_classes = 5  # 'dhp', 'sab', 'fbh', 'cfm', 'null'\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Input(shape=input_shape),  # Define input explicitly\n",
    "    Conv1D(filters=32, kernel_size=5, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    LSTM(64, return_sequences=True),  # Keep time dimension\n",
    "    LSTM(32),  # Reduce to vector\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')  # 5 output classes\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c28287d7-3495-4eee-be1d-0d7d2bfb155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 592ms/step - accuracy: 0.2309 - loss: 1.5857 - val_accuracy: 0.4000 - val_loss: 1.5148\n",
      "Epoch 2/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 553ms/step - accuracy: 0.5111 - loss: 1.5009 - val_accuracy: 0.3000 - val_loss: 1.5193\n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 555ms/step - accuracy: 0.4017 - loss: 1.4483 - val_accuracy: 0.3000 - val_loss: 1.5378\n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 591ms/step - accuracy: 0.5965 - loss: 1.3616 - val_accuracy: 0.3000 - val_loss: 1.5810\n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 552ms/step - accuracy: 0.4330 - loss: 1.3538 - val_accuracy: 0.3000 - val_loss: 1.6026\n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 553ms/step - accuracy: 0.5323 - loss: 1.2706 - val_accuracy: 0.2000 - val_loss: 1.6251\n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 552ms/step - accuracy: 0.5597 - loss: 1.2198 - val_accuracy: 0.2000 - val_loss: 1.6490\n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 553ms/step - accuracy: 0.6236 - loss: 1.1557 - val_accuracy: 0.1000 - val_loss: 1.6603\n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 562ms/step - accuracy: 0.6646 - loss: 1.0849 - val_accuracy: 0.1000 - val_loss: 1.6836\n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 556ms/step - accuracy: 0.7514 - loss: 1.0002 - val_accuracy: 0.3000 - val_loss: 1.7099\n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 567ms/step - accuracy: 0.6771 - loss: 1.0535 - val_accuracy: 0.2000 - val_loss: 1.7605\n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 573ms/step - accuracy: 0.7292 - loss: 1.0475 - val_accuracy: 0.2000 - val_loss: 1.7379\n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 551ms/step - accuracy: 0.6854 - loss: 0.9894 - val_accuracy: 0.2000 - val_loss: 1.7158\n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 555ms/step - accuracy: 0.8583 - loss: 0.7843 - val_accuracy: 0.3000 - val_loss: 1.5736\n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 553ms/step - accuracy: 0.8066 - loss: 0.7142 - val_accuracy: 0.2000 - val_loss: 1.5829\n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 556ms/step - accuracy: 0.7861 - loss: 0.7137 - val_accuracy: 0.3000 - val_loss: 1.7274\n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 561ms/step - accuracy: 0.7236 - loss: 0.6725 - val_accuracy: 0.2000 - val_loss: 1.6743\n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 558ms/step - accuracy: 0.8222 - loss: 0.6575 - val_accuracy: 0.4000 - val_loss: 1.6136\n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 558ms/step - accuracy: 0.8153 - loss: 0.7317 - val_accuracy: 0.4000 - val_loss: 1.5575\n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 585ms/step - accuracy: 0.8872 - loss: 0.5837 - val_accuracy: 0.4000 - val_loss: 1.5868\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(eeg_data, eeg_labels, epochs=20, batch_size=8, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35907c06-f7c7-40ac-8305-ea3e1e8264b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2556, 4)\n",
      "(50,)\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(eeg_data.shape)  # Should be (50, 2556, 4)\n",
    "print(eeg_labels.shape)  # Should be (50,)\n",
    "print(np.unique(eeg_labels))  # Should contain values [0, 1, 2, 3, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5c6747bd-c4b3-4a71-929b-3d3334bc5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"eeg_recording.txt\"\n",
    "eeg_file = np.loadtxt(filename, dtype=np.float32)\n",
    "\n",
    "# Define target shape\n",
    "target_samples = 2556\n",
    "num_channels = 4\n",
    "\n",
    "# Ensure it's a NumPy array\n",
    "eeg_file = np.array(eeg_file, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "013c7ec2-4f6b-4137-b9eb-ea103360dcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2557, 4)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3ca2c3a0-6880-4d89-afac-87058ceb7909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 4)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_file = eeg_file[:2556, :]  # Keep only the first 2556 rows\n",
    "eeg_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4a217a7b-9877-42f3-b082-f9eb51dc2d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2556, 4)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_file = eeg_file.reshape(1, 2556, 4)  # Add batch dimension\n",
    "eeg_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8d332efd-677d-4d8f-99b3-b7c4bef31510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "[0.06331362 0.6065506  0.08134641 0.1631499  0.08563947]\n",
      "1\n",
      "[1 3]\n",
      "Predicted Class: frustration\n",
      "Predicted Class: sadness\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(eeg_file)\n",
    "print(predictions[0])\n",
    "# Get the predicted class\n",
    "predicted_class = np.argmax(predictions, axis=1)[0]  # Returns the class index\n",
    "print(predicted_class)\n",
    "# Define class labels\n",
    "top_2_indices = np.argsort(predictions[0])[-2:][::-1]\n",
    "\n",
    "print(top_2_indices)\n",
    "\n",
    "class_labels = ['relaxed', 'frustration', 'emergency trauma', 'sadness', 'null']\n",
    "\n",
    "# Print predicted label\n",
    "for i in top_2_indices:\n",
    "    print(f\"Predicted Class: {class_labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "12849b14-9c9a-4d13-849d-4b02bc9e4fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_2_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c31fdc8b-7d9f-48d3-b0c1-a81aa7a94c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(top_2_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9adc284e-1c6a-414c-8cf0-6c0b69da5ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"brain_eeg_predict.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7d13f2e8-3710-45ca-a11a-940ea3d27468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: frustration\n",
      "Predicted Class: sadness\n",
      "['frustration', 'sadness']\n",
      "## Cinematic Image Prompt:\n",
      "\n",
      "**Scene:** A lone figure stands on a precipice overlooking a vast, fog-choked valley. The sun, a dull orange orb, is setting, casting long, distorted shadows across the landscape. The figure is silhouetted against the dying light, their features obscured. \n",
      "\n",
      "**Atmosphere:** The air hangs heavy with a palpable sense of melancholy and simmering frustration. The silence is broken only by the mournful cry of a distant bird and the low rumble of thunder in the distance, hinting at an approaching storm. \n",
      "\n",
      "**Background Details:** The valley below is a tapestry of decaying beauty. Twisted, skeletal trees claw at the sky, remnants of a once vibrant forest. Crumbling ruins of buildings peek through the fog, whispers of a forgotten past. A single, winding road disappears into the mist, leading towards the setting sun.\n",
      "\n",
      "**Character:**  The figure's posture is one of defiant stillness, their shoulders squared despite the weight of their unseen burden.  A single tear tracks down their cheek, catching the last rays of sunlight.  They clutch a worn photograph in their hand, the image barely visible.  \n",
      "\n",
      "**Additional Figures (Optional):** Two other figures, barely discernible shapes, stand further back from the precipice.  Their postures mirror the lone figure's, suggesting a shared history and a common grief.\n",
      "\n",
      "**Style:** The scene should be rendered in a muted color palette dominated by grays, blues, and oranges.  The lighting is dramatic and chiaroscuro, emphasizing the contrast between light and shadow, hope and despair. The overall effect should be one of haunting beauty and quiet contemplation. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"GEMINI API KEY\")\n",
    "\n",
    "top_emotions = []\n",
    "confidence = []\n",
    "for i in top_2_indices:\n",
    "    print(f\"Predicted Class: {class_labels[i]}\")\n",
    "    top_emotions.append(class_labels[i])\n",
    "    confidence.append((predictions[0][i])*100)\n",
    "\n",
    "print(top_emotions)\n",
    "# Construct prompt request\n",
    "prompt = (\n",
    "    f\"Generate a cinematic image prompt based on the following emotions:\\n\"\n",
    "    f\"- {top_emotions[0]} with confidence {confidence[0]}%\"\n",
    "    f\"- {top_emotions[1]} with confidence {confidence[1]}%\"\n",
    "    \"The image should depict a person\"\n",
    "    \"The background should convey more emotion than the person, using lighting, scenery, and atmosphere to enhance the mood. The backgrond should be slightly exaggerated \"\n",
    "    \"Ensure the scene is visually powerful and storytelling-driven, without any text in the image. make what they are thinking of related to the past like PTSD. make multiple people in the scene from 1 to 4\"\n",
    "    \"If prompt is relaxed then make style very calm\"\n",
    "    \"it doesnt always have to be sad depends on emotions and confidence. Depending on the confindence stress on that emotion\"\n",
    ")\n",
    "\n",
    "# Generate response from Gemini\n",
    "response = genai.GenerativeModel(\"gemini-1.5-pro-001\").generate_content(prompt)\n",
    "generated_prompt = response.text\n",
    "\n",
    "print(generated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6ac14b38-79a2-44ef-87f1-6eb8bc5dd9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-TvBsULInBzYXjejn4vd3AiEy/user-kPhXGRO88DQJtMg5Vwl4hChs/img-GvUboxHwxtHfAoxa78B7VuCi.png?st=2025-03-01T15%3A58%3A56Z&se=2025-03-01T17%3A58%3A56Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-28T18%3A43%3A52Z&ske=2025-03-01T18%3A43%3A52Z&sks=b&skv=2024-08-04&sig=VeHkaYV%2B64dvnZ6ahfYk6kDPrI0k8xBnU4P27jFPpFU%3D\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"OPENAI KEY\"\n",
    "\n",
    "response = openai.images.generate(\n",
    "    model=\"dall-e-3\",  # Use \"dall-e-2\" if you want a faster option\n",
    "    prompt=f\"{generated_prompt}\",\n",
    "    size=\"1024x1024\",\n",
    "    n=1\n",
    ")\n",
    "\n",
    "image_url = response.data[0].url\n",
    "print(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43374e-6762-4a05-957e-a61379b9823a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
